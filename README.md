<h1 align="center">üìä LiftOff Data</h1> 
<div align="center">
    <a href="https://www.python.org/" target="_blank"><img src="https://img.shields.io/badge/Python-14354C?style=for-the-badge&logo=python&logoColor=white" target="_blank"></a>
    <a href="https://www.postgresql.org/docs/" target="_blank"><img src="https://img.shields.io/badge/postgres-%23316192.svg?style=for-the-badge&logo=postgresql&logoColor=white" target="_blank"></a>
    <a href="https://fastapi.tiangolo.com/" target="_blank"><img src="https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&logo=fastapi" target="_blank"></a>
    <a href="https://streamlit.io/" target="_blank"><img src="https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white" target="_blank"></a>
    <a href="https://www.sqlalchemy.org/" target="_blank"><img src="https://img.shields.io/badge/SQLAlchemy-323232?style=for-the-badge&logo=sqlalchemy&logoColor=white" target="_blank"></a>
    <a href="https://pydantic-docs.helpmanual.io/" target="_blank"><img src="https://img.shields.io/badge/Pydantic-3776AB?style=for-the-badge&logo=pydantic&logoColor=white" target="_blank"></a>
    <a href="https://docs.docker.com/" target="_blank"><img src="https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white" target="_blank"></a>
</div>


> Este projeto apresenta uma arquitetura de pipeline de dados de baixo custo, projetada para startups que precisam processar e analisar dados de vendas de forma eficiente.

---

### **Introdu√ß√£o**

Este projeto descreve uma arquitetura de pipeline de dados de baixo custo voltada para startups, com foco em integra√ß√£o de dados de vendas a partir de APIs e CRMs, utilizando tecnologias modernas e acess√≠veis. O objetivo √© criar uma solu√ß√£o escal√°vel para ingest√£o, transforma√ß√£o e visualiza√ß√£o de dados, garantindo que tanto engenheiros de dados quanto analistas possam colaborar eficientemente. A arquitetura proposta inclui a divis√£o do pipeline em m√∫ltiplas camadas (Bronze, Silver e Gold), integra√ß√£o com APIs, Airbyte para ingest√£o de dados, Airflow para orquestra√ß√£o e DBT para transforma√ß√£o de dados. A plataforma colaborativa "Briefer" tamb√©m √© integrada, permitindo que analistas de dados acessem e utilizem os dados transformados de forma eficiente.

<p align="center">
<img src = "./img/arquitetura_1.5.png">
</p>

#### **Assistente IA Especialista em Analise de Dados e Vendas**
Al√©m do pipeline de dados, o projeto inclui uma interface de assistente AI no Streamlit, que possibilita interagir com um modelo de IA, como ChatGPT ou Llama3. Essa interface oferece uma experi√™ncia pr√°tica para an√°lise de vendas e insights, permitindo que analistas fa√ßam perguntas e obtenham respostas r√°pidas e insights relevantes.

<p align="center">
<img src = "./img/top3_vendedores.png">
</p>

#### **Dashboard Interativo**

Como parte da solu√ß√£o, desenvolvemos um **dashboard interativo** utilizando o Streamlit, que permite visualizar e analisar os dados de vendas e recursos humanos de forma intuitiva e eficiente. O dashboard apresenta m√©tricas-chave, gr√°ficos interativos e tabelas informativas, facilitando a tomada de decis√µes baseadas em dados.

**Principais funcionalidades do dashboard:**

- **Vendas ao Longo do Tempo:** Gr√°fico que mostra a evolu√ß√£o das vendas, ajudando a identificar tend√™ncias e sazonalidades.
- **Vendas por Produto:** An√°lise dos produtos mais vendidos, destacando quais geram mais receita.
- **Top 10 Melhores Vendedores:** Ranking dos vendedores com maior volume de vendas, reconhecendo a performance individual.
- **Folha Salarial Mensal:** Acompanhamento dos custos mensais com sal√°rios, monitorando a evolu√ß√£o da folha de pagamento.
- **Distribui√ß√£o de Funcion√°rios por G√™nero:** Visualiza√ß√£o da composi√ß√£o da equipe, promovendo insights sobre diversidade.
- **M√©dia Salarial por Cargo:** Compara√ß√£o das remunera√ß√µes m√©dias entre diferentes posi√ß√µes na empresa.
- **Contrata√ß√µes por M√™s:** Gr√°fico que evidencia o ritmo de crescimento da equipe ao longo do tempo.
- **Aniversariantes do M√™s:** Lista dos funcion√°rios que fazem anivers√°rio no m√™s atual, fortalecendo o engajamento interno.

<p align="center">
<img src = "./img/dashboard.png">
</p>

O dashboard foi projetado para ser acess√≠vel e f√°cil de usar, permitindo que membros da equipe sem conhecimento t√©cnico aprofundado possam explorar e interpretar os dados dispon√≠veis.

### **Sequence Diagram**

O diagrama abaixo ilustra a intera√ß√£o entre as principais camadas e componentes da arquitetura, desde a ingest√£o dos dados brutos at√© sua transforma√ß√£o e disponibiliza√ß√£o para an√°lise.
```mermaid
sequenceDiagram
    participant U as Usu√°rio
    participant SW as Sistema Web Streamlit
    participant V as Valida√ß√£o Pydantic
    participant DB as Banco de Dados SQLAlchemy
    
    U ->> SW: Inserir Dados
    SW ->> V: Enviar Dados para Valida√ß√£o
    alt Dados V√°lidos
        V ->> DB: Inserir Dados no Banco de Dados
        DB ->> SW: Confirma√ß√£o de Armazenamento
        SW ->> U: Dados Salvos com Sucesso
    else Dados Inv√°lidos
        V ->> SW: Retornar Erros de Valida√ß√£o
        SW ->> U: Exibir Mensagem de Erro
    end
```

O diagrama a seguir descreve o fluxo de dados desde a entrada do usu√°rio no frontend at√© a valida√ß√£o dos dados e o salvamento no banco de dados, se aprovado.

```mermaid
graph TD
    A[Usu√°rio Digita no Frontend] --> B{Valida√ß√£o do Contrato de Dados}
    
    B -- Dados V√°lidos --> C[Salva no Banco de Dados]
    B -- Dados Inv√°lidos --> D[Erro de Valida√ß√£o Exibido]

    A --> |Entrada de Dados| B
    B --> |Valida√ß√£o pelo Pydantic| C
    B --> |Falha na Valida√ß√£o| D
    C --> E[Dados Salvos com Sucesso]
    D --> F[Mostrar Mensagem de Erro no Frontend]
```

---

### **Tecnologias Utilizadas**

#### PostgreSQL com pg_duckdb
- **Descri√ß√£o:** O PostgreSQL √© um dos bancos de dados relacionais de c√≥digo aberto mais robustos e amplamente adotados no mercado. Com a extens√£o pg_duckdb, ele ganha a capacidade de executar consultas anal√≠ticas complexas em um armazenamento colunar otimizado, combinando a escalabilidade e a confiabilidade do PostgreSQL com a performance anal√≠tica da DuckDB.
- **Uso no Projeto:** Utilizado como banco de dados principal e para opera√ß√µes anal√≠ticas de grande volume, permitindo que consultas intensivas sejam processadas de forma mais r√°pida e eficiente.

#### FastAPI com Swagger
- **Descri√ß√£o:** FastAPI √© um framework moderno e de alto desempenho para a constru√ß√£o de APIs com Python 3.6+ baseado em tipos de dados. Ele √© projetado para ser r√°pido e f√°cil de usar, oferecendo valida√ß√£o autom√°tica de dados e documenta√ß√£o interativa.
- **Uso no Projeto:** Utilizado para criar a API que manipula dados de funcion√°rios, produtos e vendas. O FastAPI gera automaticamente a documenta√ß√£o da API utilizando Swagger, permitindo que os desenvolvedores testem as endpoints diretamente pela interface.
- **Acessando o Swagger:** Ap√≥s iniciar a aplica√ß√£o com o comando `uvicorn main:app --reload`, a documenta√ß√£o do Swagger pode ser acessada em `http://127.0.0.1:8000/docs`.

#### **Streamlit**

- **Descri√ß√£o:** Streamlit √© uma biblioteca Python de c√≥digo aberto que permite a cria√ß√£o de aplicativos web interativos de forma r√°pida e f√°cil. Utilizado principalmente para construir dashboards e interfaces de dados, o Streamlit √© ideal para prototipagem r√°pida e visualiza√ß√£o de dados sem a necessidade de conhecimentos avan√ßados em desenvolvimento web.
- **Uso no Projeto:** Utilizado para construir o frontend da aplica√ß√£o, permitindo que os usu√°rios insiram dados de vendas, fornecedores, funcion√°rios e produtos de forma interativa e visualizem os resultados diretamente na interface, por meio de um dashboard.

<p align="center">
  <img src = "./img/crud_vendas_1.1.png">
</p>

#### **Pydantic**

- **Descri√ß√£o:** Pydantic √© uma biblioteca de valida√ß√£o de dados que utiliza modelos baseados em classes Python para garantir que os dados inseridos estejam no formato correto. √â amplamente utilizada para valida√ß√£o e serializa√ß√£o de dados, garantindo integridade e consist√™ncia.
- **Uso no Projeto:** Pydantic √© utilizado para validar os dados inseridos pelos usu√°rios no frontend, garantindo que as informa√ß√µes estejam corretas antes de serem processadas e salvas no banco de dados.

#### **Psycopg2**

- **Descri√ß√£o:** Psycopg2 √© uma biblioteca que permite a intera√ß√£o com bancos de dados PostgreSQL diretamente atrav√©s de Python, facilitando a execu√ß√£o de comandos SQL e o gerenciamento das conex√µes.
- **Uso no Projeto:** Utilizado para conectar a aplica√ß√£o ao banco de dados PostgreSQL, executar comandos SQL, e salvar os dados validados.

#### **SQLAlchemy**

- **Descri√ß√£o:** SQLAlchemy √© uma poderosa biblioteca de SQL toolkit e ORM (Object-Relational Mapping) para Python. Ele permite a intera√ß√£o com bancos de dados relacionais de forma mais intuitiva, utilizando objetos Python em vez de comandos SQL diretamente.
- **Uso no Projeto:** SQLAlchemy foi utilizado para gerenciar a conex√£o com o banco de dados PostgreSQL e facilitar as opera√ß√µes de CRUD.

#### **MkDocs**

- **Descri√ß√£o:** MkDocs √© uma ferramenta est√°tica de documenta√ß√£o em Python que permite a cria√ß√£o de sites de documenta√ß√£o de forma simples e estruturada. √â especialmente √∫til para projetos que precisam de uma documenta√ß√£o clara e acess√≠vel para os desenvolvedores e usu√°rios.
- **Uso no Projeto:** MkDocs √© utilizado para gerar a documenta√ß√£o do sistema, detalhando como o projeto foi estruturado, as funcionalidades desenvolvidas, e como o sistema deve ser mantido e atualizado.
- **Configura√ß√£o e Execu√ß√£o:**
  1. Para visualizar a documenta√ß√£o localmente, execute: `mkdocs serve`
  2. Acesse a documenta√ß√£o em: `http://127.0.0.1:8000/`

<p align="center">
  <img src = "./img/mkdocs.png">
</p>

#### **MinIO**
- **Descri√ß√£o:** MinIO √© uma solu√ß√£o de armazenamento de objetos de c√≥digo aberto compat√≠vel com o protocolo Amazon S3, ideal para o armazenamento escal√°vel de grandes volumes de dados. Ele permite o uso local de servi√ßos de armazenamento distribu√≠do para dados n√£o estruturados, como arquivos, logs e objetos de dados.
- **Uso no Projeto:** No contexto deste projeto, MinIO √© utilizado para armazenar dados brutos de produtos, funcionarios e fornecedores, por exempo: imagens, documentos em pdf. Ele serve como a camada de armazenamento persistente dos dados nas fases de ingest√£o e transforma√ß√£o.

#### **Airbyte**

- **Descri√ß√£o:** Airbyte √© uma plataforma de integra√ß√£o de dados de c√≥digo aberto que permite conectar facilmente APIs, bancos de dados e outros sistemas para ingest√£o de dados em tempo real.
- **Uso no Projeto:** Respons√°vel pela ingest√£o de dados a partir de diferentes APIs e fontes de dados, garantindo que os dados sejam movidos para as camadas corretas do pipeline.

#### **Airflow**

- **Descri√ß√£o:** Apache Airflow √© uma plataforma de orquestra√ß√£o de workflows que permite o agendamento e monitoramento de pipelines de dados.
- **Uso no Projeto:** Orquestra a execu√ß√£o de todos os componentes do pipeline, desde a ingest√£o at√© a transforma√ß√£o dos dados.

#### **DBT**

- **Descri√ß√£o:** DBT (Data Build Tool) √© uma ferramenta de transforma√ß√£o de dados que permite a constru√ß√£o de modelos SQL e a aplica√ß√£o de boas pr√°ticas de desenvolvimento de software ao ETL.
- **Uso no Projeto:** Utilizado para transformar os dados das camadas Bronze e Silver, preparando-os para a camada Gold, onde estar√£o prontos para consumo pelos analistas.

<p align="center">
<img src = "./img/lineage.png">
</p>

#### **Briefer**

- **Descri√ß√£o:** Briefer √© uma plataforma colaborativa de dados que permite que equipes de analistas acessem, compartilhem e analisem dados de maneira colaborativa.
- **Uso no Projeto:** Facilita o acesso e a explora√ß√£o dos dados transformados pelos analistas, proporcionando um ambiente colaborativo para an√°lise de dados.

<p align="center">
<img src = "./img/briefer.png">
</p>

#### **n8n**

- **Descri√ß√£o:** n8n √© uma plataforma de automa√ß√£o de c√≥digo aberto que permite criar workflows integrando diferentes ferramentas e servi√ßos. Com uma interface visual intuitiva, √© poss√≠vel configurar processos complexos sem necessidade de codifica√ß√£o extensiva.
- **Uso no Projeto:** Utilizado para criar um fluxo automatizado que:
  1. Executa uma query no PostgreSQL para gerar uma audi√™ncia personalizada baseada nos produtos vendidos.
  2. Atrav√©s de um Assistente IA com modelo chatpgt 4, √© gerado um copy personalizado para o email marketing da audi√™ncia gerado no passo anterior.
  2. Envia informa√ß√µes de audi√™ncia personalizada diretamente para WhatsApp, Telegram e e-mail, facilitando a comunica√ß√£o e engajamento com clientes.

<p align="center">
<img src = "./img/n8n_fluxo_1.1.png">
</p>

### Descri√ß√£o da Audi√™ncia Personalizada:

Essa audi√™ncia identifica clientes com base em suas compras recentes (√∫ltimos 3 dias) e recomenda o produto mais vendido na mesma categoria, excluindo o produto que j√° adquiriram. O objetivo √© alavancar vendas adicionais por meio de recomenda√ß√µes personalizadas, utilizando insights das tend√™ncias de vendas dentro de categorias espec√≠ficas.

A l√≥gica utilizada para gerar essa audi√™ncia √© implementada na seguinte query:

```sql
WITH category_top_products AS (
    SELECT 
        p.categoria,
        s.name_product,
        p.description,
        p.price,
        COUNT(s.id) AS total_sales
    FROM sales AS s
    INNER JOIN products AS p ON p.name = s.name_product
    WHERE s.date >= CURRENT_DATE - INTERVAL '3 month'
    GROUP BY p.categoria, s.name_product, p.description, p.price
    ORDER BY p.categoria, total_sales DESC
),
customer_last_purchase AS (
    SELECT 
        s.email_customer,
        s.first_name,
        s.last_name,
        s.phone_number,
        s.name_product AS last_purchased_product,
        p.categoria AS last_purchased_category
    FROM sales AS s
    INNER JOIN products AS p ON p.name = s.name_product
    WHERE s.date >= CURRENT_DATE - INTERVAL '3 days'
),
recommended_products AS (
    SELECT 
        clp.email_customer,
        clp.first_name,
        clp.last_name,
        clp.phone_number,
        clp.last_purchased_product,
        clp.last_purchased_category,
        ctp.name_product AS recommended_product,
        ctp.description AS recommended_description,
        ctp.price AS recommended_price
    FROM customer_last_purchase AS clp
    INNER JOIN category_top_products AS ctp 
        ON clp.last_purchased_category = ctp.categoria
        AND clp.last_purchased_product <> ctp.name_product
)
SELECT DISTINCT ON (email_customer) *
FROM recommended_products
ORDER BY email_customer, recommended_price DESC;
```

Email de exemplo enviado ao cliente com uma recomenda√ß√£o personalizada utilizando n8n e Assistente IA especializado em Copy e Email Marketing:
<p align="center">
<img src = "./img/n8n_email_1.1.png">
</p>

### Import√¢ncia:

1. **Personaliza√ß√£o**: Oferece uma experi√™ncia personalizada ao cliente, aumentando a probabilidade de engajamento e compra.
2. **Maximiza√ß√£o de Vendas**: Promove produtos populares dentro da mesma categoria, otimizando o cross-selling e impulsionando a receita.
3. **Insights Baseados em Dados**: Utiliza tend√™ncias de vendas hist√≥ricas para identificar produtos com maior potencial de sucesso.
4. **Satisfa√ß√£o do Cliente**: Refor√ßa a percep√ß√£o de valor da marca ao sugerir produtos alinhados √†s prefer√™ncias dos clientes.

Essa abordagem √© essencial para neg√≥cios que desejam aumentar a convers√£o e fortalecer a fidelidade dos clientes, aproveitando dados hist√≥ricos e padr√µes de comportamento.

---

### **Estrutura do Projeto**

#### **Divis√£o dos M√≥dulos**

O projeto est√° dividido em m√≥dulos para organizar melhor o desenvolvimento e facilitar a manuten√ß√£o futura. A seguir, est√£o os principais m√≥dulos do projeto:

1. **Frontend (`app.py`):**
   - Respons√°vel pela interface do usu√°rio onde os dados de vendas s√£o inseridos e exibidos.
   - Desenvolvido com Streamlit para proporcionar uma intera√ß√£o simples e amig√°vel.

2. **Contrato (`<model_name>_schema.py.py`):**
   - Define as regras de valida√ß√£o dos dados utilizando Pydantic.
   - Assegura que os dados inseridos no frontend est√£o no formato correto e cumprem as regras estabelecidas pelo sistema.

3. **Banco de Dados (`database.py`):**
   - Gerencia a conex√£o e as opera√ß√µes com o banco de dados PostgreSQL utilizando Psycopg2.
   - Facilita a intera√ß√£o com o banco sem a necessidade de escrever SQL diretamente.

### Diagrama de Fluxo das Camadas Bronze, Silver e Gold no DBT

O pipeline de dados √© dividido em tr√™s camadas principais para garantir a qualidade e integridade dos dados √† medida que eles progridem no sistema:

```mermaid
graph TD
    A[Raw Data Source] -->|Extrai dados brutos| B[Bronze Layer]
    B[Bronze Layer] -->|Limpeza de dados| C[Silver Layer]
    C[Silver Layer] -->|Agrega√ß√£o e c√°lculos| D[Gold Layer - Vendas por Produto]
    C[Silver Layer] -->|Agrega√ß√£o e c√°lculos| E[Gold Layer - Vendas por Vendedor]
    
    subgraph Bronze
        B1[bronze_vendas.sql]
    end
    
    subgraph Silver
        S1[silver_vendas.sql]
    end
    
    subgraph Gold
        G1[gold_vendas_7_dias.sql]
        G2[gold_vendas_por_vendedor.sql]
    end
```

### Explica√ß√£o do Diagrama

- **Bronze Layer:** Esta camada recebe os dados brutos diretamente das fontes, criando uma visualiza√ß√£o inicial sem transforma√ß√µes significativas.
  
- **Silver Layer:** Nesta etapa, os dados s√£o limpos, ajustando datas inv√°lidas e removendo outliers. √â a fase em que os dados come√ßam a ser preparados para an√°lise.

- **Gold Layer:** Dados finais prontos para an√°lise e visualiza√ß√£o, acess√≠veis por ferramentas como o Briefer.
  - **Gold Vendas por Produto:** Agrega e calcula os dados para apresentar o desempenho dos produtos nos √∫ltimos 7 dias.
  - **Gold Vendas por Vendedor:** Apresenta o desempenho dos vendedores, tamb√©m focando nos √∫ltimos 7 dias.

---

### **Passos para Configura√ß√£o e Execu√ß√£o**

## Instala√ß√£o via Docker
1. Antes de rodar o Docker, crie um arquivo `.env` na raiz do projeto com os seguintes valores:

```plaintext
DB_HOST_PROD = postgres
DB_PORT_PROD = 5432
DB_NAME_PROD = mydatabase
DB_USER_PROD = user
DB_PASS_PROD = password
PGADMIN_EMAIL = email_pgadmin
PGADMIN_PASSWORD = password_pgadmin

OPENAI_API_KEY= api_key
GROQ_API_KEY= api_key
```

2. Para iniciar a aplica√ß√£o, execute:

```bash
docker-compose up -d --build
```

### **Gera√ß√£o de Dados Fake e Inser√ß√£o no Banco de Dados**
Este projeto inclui um pipeline para gera√ß√£o e inser√ß√£o de dados fict√≠cios de forma automatizada:
- **Gera√ß√£o de dados com Faker**: os scripts utilizam a biblioteca Faker para criar dados de teste em escala realista para v√°rias tabelas de neg√≥cios, incluindo `employees`, `products`, `sales`, e `suppliers`.
- **Inser√ß√£o no PostgreSQL**: os dados gerados s√£o salvos em arquivos Parquet e carregados diretamente no banco de dados PostgreSQL usando DuckDB, com ajuste de sequ√™ncias para prevenir problemas de IDs duplicados.

#### **Passos para Gerar e Inserir os Dados no Banco de Dados**

1. Acesse o container `backend` para executar os scripts de gera√ß√£o e carregamento de dados:
    ```bash
    docker-compose exec backend sh
    ```

2. Execute os seguintes comandos para gerar e inserir os dados:

    - Gerar dados em formato Parquet:
        ```bash
        python generate_dataset/generate_raw.py
        ```

    - Inserir os dados gerados no PostgreSQL:
        ```bash
        python generate_dataset/load_raw_to_postgres.py
        ```

3. **Executar DBT para Processamento de Dados**  
   Para rodar o DBT (Data Build Tool) e transformar os dados, execute:
   ```bash
   cd app/backend/data_warehouse/
   dbt run # ou dbt --log-level debug run
   ```

4. **Gerar Arquivos para o Assistente OpenAI**  
   Para extrair dados e criar arquivos JSON que ser√£o utilizados pelo assistente OpenAI:
   ```bash
   docker-compose exec frontend sh
   python AI/extract_data_json.py
   python AI/create_assistent_exemplo.py
   ```

5. **Criar o Assistente de IA na OpenAI**
Para configurar o projeto e permitir que ele se comunique com o assistente de IA, √© necess√°rio criar um assistente personalizado na plataforma da OpenAI e recuperar o `ASSISTANT_ID`. Este ID ser√° usado para vincular o c√≥digo ao assistente que voc√™ criou, garantindo que todas as funcionalidades de IA estejam operacionais.

- **Passo a Passo:**
  1. Acesse a plataforma da OpenAI e navegue at√© a se√ß√£o de **Assistentes de IA**.
  2. Crie um novo assistente com as configura√ß√µes desejadas, de acordo com os requisitos do projeto. (fa√ßa o upload dos arquivos .json tamb√©m)
  3. Ao finalizar a cria√ß√£o, copie o `ASSISTANT_ID` fornecido.

- **Configura√ß√£o do C√≥digo:**
  1. Abra o arquivo `app/frontend/AI/main.py`.
  2. Localize a vari√°vel `ASSISTANT_ID` e substitua o valor pelo `ASSISTANT_ID` copiado da plataforma da OpenAI.
  
  Exemplo:
  ```python
  ASSISTANT_ID = "seu_assistant_id_aqui"
  ```

#### **Prompt do Assistente de Vendas**

> **Assistente Especializado em Vendas**  
> Voc√™ √© um especialista em an√°lise e insights para equipes de vendas, focado em otimizar o desempenho de vendas, gerenciar estoques de maneira eficiente e melhorar o relacionamento com fornecedores. Sua expertise abrange a an√°lise de dados de produtos, avalia√ß√£o de desempenho de vendedores, produtividade dos funcion√°rios e gest√£o de fornecedores.
>
> Para cada solicita√ß√£o, responda com um resumo direto e insights pr√°ticos, seguidos de recomenda√ß√µes detalhadas.  
> A linguagem deve ser profissional, objetiva e pr√°tica, utilizando gr√°ficos e visualiza√ß√µes sempre que poss√≠vel para facilitar a compreens√£o dos dados.

---

### **6. Abrir a Interface do Assistente AI no Streamlit**  
   Para acessar a interface do assistente especializada em an√°lise de vendas e insights:
   ```bash
   streamlit run app/frontend/AI/main.py
   ```

### **7. Executar o Frontend**

Este projeto oferece uma interface visual desenvolvida em Streamlit, onde √© poss√≠vel visualizar e interagir com os dados carregados.

- **Comando para iniciar o frontend no Streamlit** (dispon√≠vel em [http://localhost:8501/](http://localhost:8501/)):
```bash
streamlit run app/frontend/app.py
```

--- 


### **Conclus√£o**

Este projeto de arquitetura de pipeline de dados para startups oferece uma solu√ß√£o eficiente, escal√°vel e de baixo custo para lidar com o processamento e an√°lise de grandes volumes de dados de vendas. Com a utiliza√ß√£o de ferramentas modernas como Airbyte, Airflow, DBT e Briefer, o pipeline garante a ingest√£o, transforma√ß√£o e disponibiliza√ß√£o dos dados em camadas organizadas (Bronze, Silver, Gold), permitindo uma an√°lise colaborativa e em tempo real.

Essa arquitetura modular e flex√≠vel facilita a adapta√ß√£o e o crescimento conforme a demanda aumenta, tornando-se uma excelente escolha para startups que precisam otimizar seus processos de dados sem comprometer o or√ßamento.

